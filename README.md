# Specter
SPECTER: Document-level Representation Learning using Citation-informed Transformers

# Baseline 
I have run the project successfully with following the commands 
which is provided in the Github Repository for the implementation of SPECTER 
through Huggingface Transformers Library

# Both provided 2 extension examples are implemented only with
# small size of dataset examples


# Extension 1 - Extension type: Ablation study
Performing an ablation study for the Document-level Representation Learning
using Citation-informed Transformers in the SPECTER module 
would involve systematically removing or disabling specific components 
to evaluate their impact on the model's performance. 
Here's an example code implementation for conducting an ablation study in the extension1 folder


# Extension 2 - Extension type: Domain adaptation
Domain adaptation refers to the process of adapting a model trained on a source domain
to perform well on a target domain. The goal is to leverage the knowledge learned 
from the source domain to improve the model's performance on the target domain,
even when the data distributions or characteristics of the two domains may differ.
Overall, domain adaptation in SPECTER involves fine-tuning the model 
to leverage the knowledge from a source domain and effectively utilize the citation-informed transformers
to adapt to the characteristics and requirements of a target domain.

