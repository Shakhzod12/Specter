{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To perform domain adaptation using the SPECTER module with document-level representation learning and citation-informed transformers, you can follow these steps:\n",
    "#\n",
    "# Prepare the Data:\n",
    "#\n",
    "# Collect or create datasets for the source and target domains. Each dataset should include texts, citation information, and corresponding labels (if available).\n",
    "# Split the datasets into training and testing sets for both the source and target domains.\n",
    "# Load the SPECTER Model:\n",
    "#\n",
    "# Load the pre-trained SPECTER model and tokenizer from the Hugging Face Transformers library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "specter_model = BertModel.from_pretrained('allenai/specter')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare the Dataset and DataLoader:\n",
    "# Create custom dataset classes to handle the source and target domain data. Tokenize the texts and citation information using the tokenizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DomainDataset(Dataset):\n",
    "    def __init__(self, texts, citations, labels=None):\n",
    "        self.texts = texts\n",
    "        self.citations = citations\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        citation = self.citations[idx]\n",
    "        label = self.labels[idx] if self.labels is not None else None\n",
    "\n",
    "        # Tokenize text and citation\n",
    "        input_ids = self.tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "        citation_ids = self.tokenizer.encode(citation, add_special_tokens=True, truncation=True, max_length=128)\n",
    "\n",
    "        # Create attention masks\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        citation_mask = [1] * len(citation_ids)\n",
    "\n",
    "        # Pad input sequences\n",
    "        input_padding_length = 512 - len(input_ids)\n",
    "        citation_padding_length = 128 - len(citation_ids)\n",
    "        input_ids += [0] * input_padding_length\n",
    "        citation_ids += [0] * citation_padding_length\n",
    "        attention_mask += [0] * input_padding_length\n",
    "        citation_mask += [0] * citation_padding_length\n",
    "\n",
    "        if label is not None:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(input_ids),\n",
    "                'attention_mask': torch.tensor(attention_mask),\n",
    "                'citation_ids': torch.tensor(citation_ids),\n",
    "                'citation_mask': torch.tensor(citation_mask),\n",
    "                'label': torch.tensor(label)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(input_ids),\n",
    "                'attention_mask': torch.tensor(attention_mask),\n",
    "                'citation_ids': torch.tensor(citation_ids),\n",
    "                'citation_mask': torch.tensor(citation_mask)\n",
    "            }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the Domain Adaptation Model:\n",
    "# Create a domain adaptation model that incorporates the document-level representations from SPECTER and the citation information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DomainAdaptationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DomainAdaptationModel, self).__init__()\n",
    "        self.specter = specter_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.citation_fc = nn.Linear(768, 256)  # Adjust dimensions as needed\n",
    "        self.classifier = nn.Linear(1024, num_classes)  # Adjust dimensions as needed\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, citation_ids, citation_mask):\n",
    "        specter_outputs = self.specter(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        specter_pooled_output = specter_outputs.pooler_output\n",
    "        citation_rep = self.specter(input_ids=citation_ids, attention_mask=citation_mask).pooler_output\n",
    "        citation_rep = self.dropout(citation_rep)\n",
    "        citation_representation = self.citation_fc(citation_rep)\n",
    "        combined_representation = torch.cat((specter_pooled_output, citation_representation), dim=1)\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and Evaluate the Domain Adaptation Model:\n",
    "# Initialize the domain adaptation model, define the loss function and optimizer, and train the model on the source domain.\n",
    "# Perform domain adaptation by training the model on the target domain with and without labels.\n",
    "# Evaluate the model's performance on the target domain test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare source and target domain data\n",
    "source_texts = [\"Source domain text 1\", \"Source domain text 2\", \"Source domain text 3\"]\n",
    "source_citations = [\"Source domain citation 1\", \"Source domain citation 2\", \"Source domain citation 3\"]\n",
    "source_labels = [0, 1, 0]  # Example labels for source domain\n",
    "\n",
    "target_texts = [\"Target domain text 1\", \"Target domain text 2\", \"Target domain text 3\"]\n",
    "target_citations = [\"Target domain citation 1\", \"Target domain citation 2\", \"Target domain citation 3\"]\n",
    "target_labels = [1, 0, 1]  # Example labels for target domain\n",
    "\n",
    "# Create source and target domain datasets\n",
    "source_dataset = DomainDataset(source_texts, source_citations, source_labels)\n",
    "target_dataset = DomainDataset(target_texts, target_citations, target_labels)\n",
    "\n",
    "# Create source and target domain dataloaders\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=2, shuffle=True)\n",
    "target_dataloader = DataLoader(target_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Initialize domain adaptation model\n",
    "num_classes = 2  # Number of classes for classification task\n",
    "domain_model = DomainAdaptationModel(num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(domain_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    domain_model.train()\n",
    "\n",
    "    # Train on source domain\n",
    "    for batch in source_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        citation_ids = batch['citation_ids']\n",
    "        citation_mask = batch['citation_mask']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # Forward pass\n",
    "        logits = domain_model(input_ids, attention_mask, citation_ids, citation_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Perform domain adaptation on target domain\n",
    "    for batch in target_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        citation_ids = batch['citation_ids']\n",
    "        citation_mask = batch['citation_mask']\n",
    "\n",
    "        # Forward pass (without labels)\n",
    "        logits = domain_model(input_ids, attention_mask, citation_ids, citation_mask)\n",
    "\n",
    "        # Compute target domain loss (without labels)\n",
    "        target_loss = -logits.max(dim=1).values.mean()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        target_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation on target domain\n",
    "    domain_model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in target_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            citation_ids = batch['citation_ids']\n",
    "            citation_mask = batch['citation_mask']\n",
    "            labels = batch['label']\n",
    "\n",
    "            # Forward pass\n",
    "            logits = domain_model(input_ids, attention_mask, citation_ids, citation_mask)\n",
    "\n",
    "            # Compute predictions\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "\n",
    "            # Update counts\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Epoch: {epoch+1} | Target Domain Accuracy: {accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
